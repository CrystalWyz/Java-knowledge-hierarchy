## 第05章 深入Kafka

1. Kafka使用Zookeeper来维护群成员的信息。每个broker都有一个唯一标识符，这个标识符可以在配置文件里指定，也可以自动生成。在broker启动的时候，它通过创建临时节点把自己的ID注册到Zookeeper。Kafka组件订阅Zookeeper的/brokers/ids路径(broker在Zookeeper上的注册路径)，当有broker加入集群或退出集群时，这些组件就可以获得通知。

2. 在broker停机、出现网络分区或长时间垃圾回收停顿时，broker会从Zookeeper上断开连接，此时broker在启动时创建的临时节点会自动从Zookeeper上移除。监听broker列表的Kafka组件会被告知该broker已移除。

3. 在关闭broker时，它对应的节点也会消失，不过它的ID会继续存在于其他数据结构中。如果使用相同的ID启动另一个全新的broker，它会立即加入集群，并拥有与旧broker相同的分区和主题。

4. 控制器其实就是一个broker，只不过它除了具有一般broker的功能之外，还负责分区首领的选举。集群里第一个启动的broker通过在Zookeeper里创建一个临时节点/controller让自己成为控制器。其他broker在启动时也会尝试创建这个节点，不过它们会收到一个“节点已存在”异常，然后“意识”到控制器节点已存在，其他broker在控制器节点上创建Zookeeper watch对象，这样它们就可以收到这个节点的变更通知。

5. 如果控制器被关闭或者与Zookeeper断开连接，Zookeeper上的临时节点就会消失。集群里的其他broker通过watch对象得到控制器节点消失的通知，它们会尝试让自己成为新的控制器。第一个在Zookeeper里成功创建控制器节点的broker就会成为新的控制器，其他节点会收到“节点已存在”异常，然后在新的控制器节点上再次创建watch对象。每个新选出的控制器通过Zookeeper的条件递增操作获得一个全新的、数值更大的controller epoch。其他broker在知道当前controller epoch后，如果收到控制器发出的包含较旧的消息，就会忽略它们。

6. 当控制器发现一个broker已经离开集群，它就知道，那些失去首领的分区需要一个新首领。控制器遍历这些分区，并确定谁应该成为新首领（简单来说就是分区副本列表里的下一个副本），然后向所有包含新首领或现有跟随者的broker发送请求。该请求包含了谁是新首领谁是分区跟随者的信息。随后，新首领开始处理来自生产者的请求，而跟随者开始从新的首领那复制消息。

7. 当控制器发现一个broker加入集群时，它会使用broker ID来检查新加入的broker是否包含现有分区的副本。如果有，控制器就把变更通知发送给新加入的broker和其他broker，新broker上的副本开始从首领那里复制消息。

8. 简而言之，Kafka使用Zookeeper的临时节点来选举控制器，并在节点加入集群或退出集群时通知控制器。控制器负责在节点加入或离开集群时进行分区首领选举。控制器使用epoch来避免“脑裂”。“闹裂”是指两节点同时认为自己是当前的控制器。

9. 在Kafka的文档里，Kafka把自己描述成“一个分布式的、可分区的、可复制的提交日志服务”。

10. 副本有以下两种类型：

    1. 首领副本：每个分区都有一个首领副本。为了保证一致性，所有生产者请求和消费者请求都会经过这个副本。
    2. 跟随着副本：首领以外的副本都是跟随着副本。跟随着副本不处理来自客户端的请求，它们唯一的任务就是从首领那复制消息，保持与首领一致的状态。如果首领发生崩溃，其中的一个跟随着会被提升成为新首领。

11. 首领的另一个任务是搞清楚哪个跟随着的状态与自己是一致的。为了与首领保存同步，跟随着向首领发生获取数据的请求，这种请求与消费者为了读取消息而发送的请求是一样的。通过查看每个跟随着请求的最新偏移量，首领就会知道每个跟随者复制的进度。如果跟随着在10s哪没有请求任何消息，或者虽然在请求消息，但在10s内没有请求最新的数据，那么它就会被认为是不同步的。如果一个副本无法与首领保持一致，在首领发生失效时，他就不可能成为新的首领。

12. 持续请求得到的最新消息副本被称为同步的副本。在首领发生失效时，只有同步副本才有可能被选为新首领。

13. 跟随者的正常不活跃时间或在成为不同步副本之前的时间是通过replica.lag.time.max.ms参数来配置。

14. 除了当前首领之外，每个分区都有一个首选首领——创建主题是选定的首领就是分区的首选首领。默认情况下，Kafka的auto.leader.rebalance.enable被设为true，他会检查首选首领是不是当前首领，如果不是，并且该副本时同步的，那么就会触发首领选举，让首选首领成为当前首领。（从分区的副本清单里可以很容易找到首选首领，清单里的第一个副本一般就是首选首领）

15. Kafka提供了一个二进制协议（基于TCP），指定了请求消息的格式以及broker如何对请求作出响应——包括成功处理请求或在处理请求过程中遇到的错误。

16. 所有的请求消息都包含一个标准的消息头：

    1. Request type（也就是API key）
    2. Request version（broker可以处理不同版本的客户端请求，并根据客户端版本做出不同的响应）
    3. Correlation ID——一个具有唯一性的数字，用于表示请求消息，同时也会出现在响应消息和错误日志里。
    4. Client ID——用于表示发送请求的客户端。

17. broker会在它所监听的每一个端口上允许一个Acceptor线程，这个线程会创建一个连接，并把它交给Processor线程去处理。Processor线程（也被叫做“网络线程”）的数量是可配置的。网络线程负责从客户端获取请求消息，把它们放进请求队列，然后从响应队列获取响应消息，把它们发送给客户端。请求消息被放倒请求队列后，IO线程会的则处理它们。

18. 生产请求和获取请求都必须发送给分区的首领副本。（跟随者副本仅仅用于复制）如果broker收到一个针对特定分区的请求，而该分区的首领在另一个broker上，那么发送请求的客户端会收到一个“非分区首领”的错误响应。kafka客户端要自己负责把生产请求和获取请求发送到正确的broker上。

19. 元数据请求包含了客户端感兴趣的主题列表。服务器端的响应消息里指明了这些主题所包含的分区，每个分区都有哪些副本，以及哪个副本是首领。元数据请求可以发送给人以一个broker，因为所有broker都缓存了这些信息。

20. 一般情况下，客户端会把这些信息缓存起来，需要时不时地通过发送元数据请求来刷新这些信息（刷新时间间隔通过metadata.max.ages.ms参数来配置。

21. 包含首领副本的broker在受到生产请求时，会对请求做一些验证：

    1. 发送数据的用户是否有主题写入权限？
    2. 请求里包含的acks值是否有效（只允许出现0、1或all）？
    3. 如果acks=all，是否有足够多的同步副本保证消息已经被安全写入？（我们可以对broker进行配置，如果同步副本的数量不足，broker可以拒绝处理新消息）

    之后，消息被写入磁盘。

22. 在消息被写入分区的首领之后，broker开始检查acks配置的参数——如果acks被设为0或1，那么broker立即返回响应；如果acks被设为1，那么请求会被保存在一个叫做炼狱的缓冲区里，直到首领发现所有跟随着副本都复制了消息，响应才会被返回给客户端。

23. 客户端还可以指定broker最多可以从一个分区里返回多少数据。

24. Kafka使用零复制技术向客户端发送消息——Kafka直接把消息从文件（更确切的说是Linux文件系统缓存）里发送到网络通道，而不需要经过任何中间缓冲区。

25. 并不是所有保存在分区首领上的数据都可以被客户端读取。大部分客户端只能读取已经被写入所有同步副本的消息，在消息还没有被写入所有同步副本之前，是不会发送给消费者的。

26. 延迟时间可以通过参数replica.lag.time.max.ms来配置，他指定了副本在复制消息时可被允许的最大延迟时间。

27. 主题的创建需要通过命令行工具来完成，命令行工具会直接更新Zookeeper里的主题列表，broker会监听这些主题列表，在有新主题加入时，它们会收到通知。

28. Kafka的基本存储单元是分区。分区无法在多个broker间进行再细分，也无法再同一个broker的多个磁盘上进行再细分。

29. 目录分配规则：计算每个目录里的分区数量，新的分区总是被添加到数量最小的那个目录里。

30. Kafka不会一直保留数据，也不会等到所有消费者都读取了消息之后才删除消息。Kafka管理员为每个主题配置了数据保留期限，规定了数据被删除之前可以保留多长时间，或者清理数据之前可以保留的数据量大小。

31. 分区分成若干个片段。默认情况下，每个片段包含1GB或一周的数据，以较小的那个为准。在broker往分区里写数据时，如果达到片段上限，就关闭当前文件，并打开一个新文件。

32. 当前正在写入数据的片段叫作活跃片段。活跃片段永远不会删除，所以如果你要保留数据一天，但片段里包含了5天的数据，那么这些数据会被保留舞台，因为在片段被关闭之前这些数据无法被删除。

33. broker会为分区里的每个片段打开一个文件句柄，哪怕片段是不活跃的。

34. Kafka的消息和偏移量保存在文件里。保存在磁盘上的数据格式与从生产者发送过来活着发送给生产者的消息格式是一样的。因为使用了相同的数据格式进行磁盘存储和网络传输，Kafka可以使用零复制技术给消费者发送消息。

35. 除了键、值和偏移量外，消息里还包含了消息大小、校验和、消息格式版本号、压缩算法和时间戳。时间戳可以是生产者发送消息的时间，也可以是消息daodabroker的时间，这个是可配置的。

36. 如果生产者发送的是压缩过的消息，那么同一个批次的消息会被压缩在一起，被当作“包装消息”进行发送。于是，broker就会收到一个这样的消息，然后再把它发送给消费者。消费者在解压这个消息之后，会看到真个批次的消息，它们都有自己的时间戳和偏移量。

37. Kafka feudal一个叫DumpLogSegment的工具，可以用它来查看片段的内容。如果使用了--deep-iteration参数，可以显示被压缩到包装消息里的消息。

38. 消费者可以从Kafka的任意可用偏移量位置开始读取消息。为了帮助broker更快地定位到指定的偏移量，Kafka为每个分区维护了一个索引。索引把偏移量映射到片段文件和偏移量在文件里的位置。

39. 索引也被分成片段，所以在删除消息时，也可以删除相应的索引。kafka不维护索引的校验和。如果索引出现损坏，Kafka或通过重新读取消息并录制偏移量和位置来重新生成索引。如果有必要，管理员可以删除索引，Kafka会自动冲洗生成这些索引。

40. 一般情况下，Kafka会根据设置的时间保留数据，把超过时效的旧数据删除掉。只有当应用程序生成的事件里包含了键值对时，为这些主题设置compact策略才有意义。如果主题包含了null键，清理就会失败。

41. 每个日志片段可以分为以下两个部分：

    1. 干净的部分：这些消息之前被清理过，每个键只有一个对应的值。
    2. 污浊的部分：这些消息是在上一次清理之后写入的。

42. 如果在Kafka启动时启用了清理功能（通过配置log.cleaner.enabled参数），每个broker会启动一个清理管理器线程和多个清理线程，它们负责执行清理任务。这些线程会选择污浊率较高的分区进行清理。

43. 为了清理分区，清理线程会读取分区的污浊部分，并在内存里创建一个map。map里的每个元素包含了消息键的散列值和消息的偏移量，键的散列值是16B，加上偏移量总共是24B。

44. 管理员在配置Kafka时可以对map使用的内存大小进行配置。每个线程都有自己的map，而这个参数指的是所有线程可使用的内存总大小。Kafka并不要求分区的整个污浊部分来适应这个map的大小，但要求至少有一个完整的片段必须符合。如果只有少部分片段可以完全符合，Kafka将从最旧的片段开始清理，等待下一次清理剩余的部分。

45. 清理线程在创建好偏移量map后，开始从干净的片段处读取消息，从最旧的消息开始，把它们的内容与map里的内容进行比对。它会检查消息的键是否存在于map中，如果不存在，说明消息的值是最新的，就把消息复制到替换片段上。如果键已存在，消息会被忽略，因为在分区的后部已有一个具有相同键的消息存在。

46. 为了彻底把一个键从系统里删除，应用程序必须发送一个包含该键且值为null的消息。清理线程发现该消息时，会先进行常规的清理，只保留值为null的消息。该消息（被称为墓碑消息）会被保留一段时间，时间长短是可配置的。在这期间，消费者可以看到这个墓碑消息，并且发现它的值已经被删除。于是，如果消费者网数据库里复制Kafka的睡觉，当它看到这个墓碑消息时，就知道应该要把相关的用户信息从数据库里删除。在这个时间段过后，清理线程会移除这个墓碑消息。

47. compact策略也不会对当前片段进行清理，只有旧片段里的消息才会被清理。