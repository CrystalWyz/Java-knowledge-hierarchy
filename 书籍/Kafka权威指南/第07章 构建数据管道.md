## 第07章 构建数据管道

1. Kafka 作为一个基于流的数据平台， 提供了可靠且可伸缩的数据存储， 可以支持几近实时的数据管道和基于小时的批处理。
2. Kafka 本身就支持“至少一 次传递”，如果再结合具有事务模型或唯一键特性的外部存储系统， Kafka 也能实现“仅一 次传递”。因为大部分的端点都是数据存储系统，它们提供了“仅一次传递”的原语支持， 所以基于 Kafka 的数据管道也能实现“仅一次传递”。
3. 数据管道需要协调各种数据格式和数据类型，Kafka 和 Connect API 与数据格式无关（对它们来说都是无意义的字节数组）。数据池连接器将 Kafka 的数据写入外部系统，因此需要负责处理数据格式。处理好不同数据源和数据池 之间的行为差异。
4. 数据管道的构建可以分为两大阵营， 即 ETL 和 ELT：
   1. ETL 表示 提取 — 转换 — 加载 （Extract-Transform-Load）， 也就是说， 当数据流经数据管道 时，数据管道会负责处理它们。不足：数据的转换会给数据管道下游的应用造成一些 限制。
   2. ELT 表示 提取 — 加载 — 转换 （Extract-Load-Transform）。在这种模式下，数据管道只做少量的 转换（主要是数据类型转换），确保到达数据池的数据尽可能地与数据源保持一致。这种 情况也被称为高保真（high ﬁdelity）数据管道或数据湖（data lake）架构。
5. Kafka 支持加密传输数据，从数据源到 Kafka， 再从 Kafka 到数据池。它还支持认证（通过 SASL 来实现）和授权
6. 尽量保留原始数据的完整性，让下游的应用自己决定如 何处理和聚合数据。
7. Connect 是 Kafka 的一部分， 它为在 Kafka 和外部数据存储系统之间移动数据提供了一种 可靠且可伸缩的方式。它为连接器插件提供了一组 API 和一个运行时——Connect 负责运 行这些插件， 它们则负责移动数据。 Connect 以 worker 进程集群的方式运行
8. 数据源的连接器负责从源系统读取数据， 并把数据对象 提供给 worker 进程。 数据池的连接器负责从 worker 进程获取数据， 并把它们写入目标 系统。
9. Connect 进程有以下几个重要的配置参数：
   1. bootstrap.servers：该参数列出了将要与 Connect 协同工作的 broker 服务器，连接器将会向这些 broker 写入数据或者从它们那里读取数据。你不需要指定集群所有的 broker，不过建议至少指定 3 个
   2. group.id：具有相同 group id 的 worker 属于同一个 Connect 集群。集群的连接器和它们 的任务可以运行在任意一个 worker 上。
   3. key.converter 和 value.converter：Connect 可以处理存储在 Kafka 里的不同格式的 数据。 这两个参数分别指定了消息的键和值所使用的转换器。 默认使用 Kafka 提供的 JSONConverter， 当然也可以配置成 Conﬂuent Schema Registry 提供的 AvroConverter。
10. 一般通过 Connect 的 REST API 来配置和监控 rest.host.name 和 rest.port 连接器。
11. 在删除连接器之后，其他的连接器会重启它们的任务。 这是为了在 worker 进程间平衡剩余的任务
12. 连接器负责以下3件事情：
    1. 决定需要运行多少个任务。
    2. 按照任务来拆分数据复制。
    3. 从 worker 进程获取任务配置并将其传递下去。
13. 任务：任务负责将数据移入或移出 Kafka。 任务在初始化时会得到由 worker 进程分配的一个 上下文：源系统上下文（Source Context）包含了一个对象， 可以将源系统记录的偏移 量保存在上下文里
14. worker 进程是连接器和任务的“容器”。它们负责处理 HTTP 请求，这些请求用于定义连 接器和连接器的配置。它们还负责保存连接器的配置、启动连接器和连接器任务，并把配 置信息传递给任务。
15. 在设计一个源连接器时，要着重考虑如 何对源系统的数据进行分区以及如何跟踪偏移量，这将影响连接器的并行能力，也决定了 连接器是否能够实现至少一次传递或者仅一次传递。