## 第十一章 性能可伸缩性

1. 当操作性能由于某种特定的资源而受到限制的时，我们通常将该操作称为资源密集型的操作。

2. 想要通过并发来获得更好的性能，需要努力做好两件事情：更有效地利用现有处理资源，以及在出现新的处理资源时使程序尽可能地利用这些新资源。（线程并不是越多越好，额外的线程通常会引入额外的开销，这些开销可能会超过过带来的性能提升。）<font color="red">多线程的意义在于提高资源的利用率</font>

3. 可伸缩性指的是：当增加计算资源时，程序的吞吐量或者处理能力相应地增加。

4. 性能地这两个方面——“多块”和“多少”，是完全独立的，有时候甚至是相互矛盾的。大多数提高但线程程序性能的技术，往往都会破坏可伸缩性。

5. 提高可伸缩性往往会造成性能损失，相对于分层架构，单一程序通常性能更高，并且更容易遇到性能瓶颈。

6. 并发错误是最难追踪和消除的错误，因此对于任何可能会引入这类错误的措施，都需要谨慎实施。

7. 优化往往伴随着其他成本的提升。

8. 如果使用线程主要是为了发挥多个处理器的处理能力，那么就必须对问题进行合理的并行分解，并使得程序能有效地使用这种潜在的并行能力。

9. Amdahl定率：在增加计算资源的情况下，程序在理论上能过实现最高加速比。这个值取决于程序中可并行组件与串行组件所占的比重：

   ​								S <= 1 / (F + (1 - F) / N)				 F:串行部分 N:处理器数量

10. 在所有并发执行中都包含一些串行部分。

11. 如果可运行的线程数大于CPU的数量，那么操作系统最终会将某个正在运行的线程调度出来，从而使其他线程能够使用CPU。这将导致一次上下文切换，在这个过程中将保存当前运行程序的执行上下文。

12. 切换上下文需要一定时间的开销，而在线程调度过程中需要访问由操作系统和JVM共享的数据结构。应用程序、操作系统以及JVM都是用一组相同的CPU。在JVM和操作系统的代码中消耗越多的CPU时钟周期，应用程序的可用CPU时钟周期就越少。

13. 上下文切换的开销并不只是包含JVM的操作系统的开销。当一个新的线程被切换进来时，它所需要的数据可能不在当前处理器的本地缓存中，因此上下文切换将导致一些缓存缺失，因而线程在首次调度运行时会更加缓慢。（丢失局部性原理）

14. 调度器会为每个可运行的线程分配一个最小执行时间。

15. 当线程由于等待某个发生竞争的锁而被阻塞时，JVM通常会将这个线程挂起，并允许它被交换出去。如果线程频繁发生阻塞，那么它们将无法使用完整的调度时间片。在程序中发生越多的阻塞，与CPU密集型的程序就会发生越多的上下文切换，从而增加调度开销，并因此而降低吞吐量。（无阻塞苏有助于减少上下文切换）

16. 上下文切换的实际开销会随着平台的不同而变化，然后按照经验来看：在大多数通过的处理器中，上下文切换的开销相当于5000～10000个时钟周期，也就是几微秒。

17. 内存栅栏可以刷新缓存，使缓存无效，刷新硬件的写缓冲，以及停止执行管道。内存栅栏可能同样会对性能带来间接的影响，因为它们将抑制一些编译器优化操作。在内存栅栏中，大多数操作都是不能被重排序的。

18. 在评估同步操作带来的性能影响时，区分有竞争的同步和无竞争的同步非常想要，synchronized机制针对无竞争的同步进行优化，一个“快速通道”非竞争同步将消耗20～250个时钟周期。

19. 现代的JVM能通过优化来去掉一些不会发生竞争的锁，从而减少不必要的同步开销。

20. 编译器可以通过锁消除优化与锁粒度来优化同步加锁。

21. 非竞争的同步可以完全在JVM中进行处理，而竞争的同步可能需要操作系统的介入，从而增加开销。JVM在实现阻塞行为时，可以采用自选等待（指通过循环不断尝试地尝试获取锁，直到成功）或者通过挂起被阻塞的线程。

22. 如果等待时间较短，则适合采用自旋等待方式，而如果等待时间较长，则适合采用线程挂起方式。

23. 当线程无法获取某个锁或者由于某个条件等待或在I/O操作上阻塞时，需要被挂起，在这个过程中将包含两次额外的上下文切换，以及所有必要的操作系统和缓存操作：被阻塞的线程在其执行时间片还未用完之前就被交换出去，而在随后当要获取的锁或者在其他资源可用时，又再次被切换回来。（由于锁竞争而导致阻塞时，线程在持有锁时将存在一定的开销：当它释放锁时，必须告诉操作系统系统恢复运行阻塞的线程。）

24. 串行操作会降低可伸缩性，并且上下文切换也会降低性能。在锁上发生竞争时将同时导致这两种问题，因此减少锁的竞争能够提高性能和可伸缩性。

25. 在并发程序中，对可伸缩性的最主要威胁就是独占方式的资源锁。

26. 有3种方式可以降低锁的竞争程度：

    1. 减少锁的持有时间。
    2. 降低锁的请求频率。
    3. 使用带有协调机制的独占锁，这些机制允许更高的并发性。

27. 降低发生竞争可能性大一种有效方式就是尽可能缩短锁的持有时间。在实际情况中，仅当可以将一些“大量”的计算或阻塞操作从同步代码块中移出时，才应该考虑同步代码块的大小。

28. 另一种减小锁的持有时间的方式是降低线程请求锁的频率。

29. 对竞争适中的锁进行分解时，实际上是把这些锁转变为非竞争的锁，从而有效地提高性能和可伸缩性。

30. 在某些情况下，可以将锁分解技术进一步扩展为一组独立对象上的锁进行分解，这种情况被称为锁分段。锁分段的一个劣势在于：与采用单个锁来实现独占访问相比，要获取多个锁来实现独占访问将更加困难并且开销更高。

31. ConcurrentHashMap中的size将对每个分段进行枚举并将每个分段中的元素数量相加，而不是维护一个全局计数。为了避免枚举每个元素值，ConcurrentHashMap为每个分段都维护一个独立的计数，并通过每个分段的锁来维护这个值。

32. 第三种降低竞争锁的影响的技术就是放弃使用独占锁，从而有助于使用一种友好并发的方式来管理共享分享。

33. ReadWriteLock实现了一种在多个程序操作以及单个写入操作情况下的加锁规则：如果在多个读取操作都不会修改共享资源，那么这些读取操作可以同时访问该共享资源，但在执行写入操作的时候必须以独占的方式来获取锁。

34. 原子变量类提供了在整数或者对象引用上的细粒度原子操作（因此可伸缩性更高），并使用了现代处理器中提供的底层并发原语。如果在类中只包含少量的热点语。如果再泪中只包含少量的热点域，并且这些域不会与其他变量参与到不变性条件中，那么用原子变量来代替它们能提高可伸缩性。（虽然原子变量能降低热点域的更新开销，但并不能完全消除）

35. 如果所有CPU的利用率并不均匀（有些CPU在忙碌的运行，而其他CPU却并非如此），那么你的首要目标就是进一步找出程序中的并行性。

36. 在vmstat命令的输出中，有一栏信息是当前处于可运行状态但并没有运行（由于没有足够的CPU）的线程数量。如果CPU的利用率很高，并且总会有运行的线程在等待CPU，那么增加更多的处理器时，程序的性能可能会得到提升。

37. 使用对象池会引入线程对对象的竞争。

38. 在单线程环境下，ConcurrentHashMap的性能比同步的HashMap的性能略好一些，但在并发环境中则要好得多。在ConcurrentHashMap在实现中假设，大多数常用的操作都是获取某个已经存在的值，因此它对各种get操作进行了优化从而提供最高的性能和并发性。

39. 当任务在运行和阻塞这两个状态之间转换时，就相当于一次上下文切换。

40. 在代码中造成的上下文切换次数越多，吞吐量就越低。